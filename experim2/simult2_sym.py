import dataclasses
import math
import numpy as np
from scipy.interpolate import interp1d

# from typing import ...
from operator import xor


"""
    Simulates the example 1 of [1].
    [1]. Smith and Brown 2003. "Estimating a State-Space Model from Point Process Observations".
"""

# -------------------------------
# * epxeriment
#    * simulation
#       * simulation inputs
#       * simulation results
# -------------------------------

"""
Symbols legend:

    _Œû : numpy arrays of 1 dim
    _ŒûŒû : numpy arrays of 2 dims
    _Œæ  : list, or variable length

    _ë¥† : trials. Indexed by trial number.

    _Œû[neuron_id] : any array that is indexed by [neuron_id]
    _Œû[inp_id] : .... input_id

    œü : any array that is indexed by spike number (spike times, Œõ at timepoints) will be suffix-ed by œü for clarity.
    œü : Sometimes this is read as "spike". Also point (time-point) or moment in a point process (discrete-indexes list or array)

    x‚Çñ : scalar
    [‚Çñ] : Any subscript []‚Çñ is a scalar value or element (not the array). It can be wrapped in an array. eg x‚Çñ_Œû

    N·∂ú, Œõ, Œî: Œª : As used in the Mathematical notation in standard of formulation of Point Process, etc
    [‚Å±‚Åø·µõ] Œõ‚Å±‚Åø·µõ : Any (inverse) function

    The idea of using unicodes for readability of indices, etc is mine. It is really helps in readbiltiy.
"""

# simargs1: SimulatorArgs1
#         ( .Delta )
# simulation_result

# na
# get_neuron_tau

# DELTA0  ??



MSEC = 1. / 1000.


# **********************************************************************************
# *                                  neuron model
# **********************************************************************************


"""
A neurons is characterised by equations 2.2 and 2.6, as in example 1 of [1].
Ref. equations #Eq.1 and #Eq.2
"""


class Neuron_static:
    def report_neuron(n, Delta):
        tau = Neuron_static.get_neuron_tau(n, Delta)
        print('Tau=', tau * 1000.0, ' (msec)')
        print('Noisiness:  sigma_eps = ',
            n['sigma_eps'] * 1000.0, ' (milli Volts per sample)')


    def get_neuron_tau(n, Delta):
        # todo: def get_ER_tau(n, Delta, rho)  # ER: Exponential Relaxation
        tau = - Delta / math.log(n['rho'])
        return tau


    def get_neuron_rho(tau, Delta):
        rho = math.exp(- Delta / tau)
        return rho

    def n0_prot():
        CORRECT_IT = 1.0

        n0 = {
            # Latent process model
            'rho': 0.99,
            'alpha': 3.0,
            'sigma_eps': math.sqrt(0.001),  # noisiness

            # Latent-to-observable (gain), or, state-to-Lprobability
            'mu': -4.9 + CORRECT_IT*math.log(1000),
            'beta': 0.0
        }
        print(repr(n0))
        return n0

        # see describe_model_latex()


        descriptions = {
            'rho': ["", ""],
            'alpha': ["input gain", "volts per amps"],
            'sigma_eps': ["noisiness: noise amplitude", ""],

            'mu': ["", ""],
            'beta': ["", ""]
        }

# Part of the problem specs, but not the Delta used in the simulation.
DELTA0 = 1.0 * MSEC

# **********************************************************************************
# *                                  simulation
# **********************************************************************************

#simargs1  # simulation args
#simargs1.K
# simulator.K

class SimulatorArgs1(object):
    """
       `.T` Simulation duration (Time length) (all time units in seconds)
       `.K` length (bins/timesteps/intervals): int
       `.Delta`: (seconds)

       invariants:
            duration ~= K * Delta

    """
    # old incorrect comment: Simulation Time Length (Intervals)


    Delta: float
    K: int
    T: float

    def __init__(self, _K=None, duration=None, _deltaT=None):
        """
            Either based on `_K` or `duration`
            They specify the duration of simulation.
        """
        #self.Delta = 0.000
        #self.K = -1
        #self.T = 0.0000
        assert _deltaT is not None, '_deltaT: time-step (bin) size in seconds'

        if _K is not None:
            assert duration is None
            self.K = _K
            #self.Delta = self.T / float(self.K)
            #self.Delta = 1 * MSEC
            self.Delta = _deltaT
            self.T = self.K * self.Delta

            """
            elif duration is not None:
                # self.K = 3000
                # self.T = 3.0; self.Delta =  # sec
                self.T = duration
                self.Delta = 1 * MSEC  * 0.01
                self.K = int(self.T / self.Delta + 1 - 0.00001)
                print( "K=", self.K )
            """
        elif duration is not None:
            assert _K is None
            self.T = duration
            #self.Delta = 1 * MSEC * 0.2
            self.Delta = _deltaT
            self.K = int(self.T / self.Delta + 1 - 0.00001)

        else:
            assert False, "Either `_K` or `duration` needs be specified"

        print("Simulation time-steps: K=%g" % self.K)

    def invar(self):
        # simargs1.K = 3000
        # simargs1.T = 3.0; simargs1.Delta =  # sec
        # simargs1.K = int(simargs1.T / simargs1.Delta + 1)

        assert simargs1.K
        assert simargs1.Delta
        assert simargs1.T

    # produces each timestep? no longer.
    # simulate_input()
    # provides: 1. basic simulatin args (part 1), instantiates simargs1
    # also user-interface for that. Conventient providing of three items: K/dt/T
    # Could be a factory method as part of SimulatorArgs1!
    def simargs_factory(_K=None, duration=None, deltaT=None):

        assert xor(_K is None, duration is None), \
            """ Simulation duration is either based on `_K` or `duration`.
                duration ~= K * Delta
            """
        # todo: remove global
        global simargs1
        # simargs1.T = Simulation Time Length (Sec)
        assert deltaT is not None, 'deltaT: time-step (bin) size in seconds'
        simargs1 = SimulatorArgs1(_K=_K, duration=duration, _deltaT=deltaT)
        return simargs1


# old idea, occluded by the idea of `simulate_step()`:
# ... = simulate_input()



def input_I‚Çñ(recurrent_state, aux_input):
    (last_every_second,) = recurrent_state
    (t, Œît,) = aux_input
    isfirst = last_every_second is None

    if isfirst:
        last_every_second = -float('inf')

    every_second = (t) % 1.0
    fire = every_second < last_every_second
    if fire:
        I‚Çñ = 1.0
    else:
        I‚Çñ = 0.0

    # What is this? Is it Dirac? If so, why not multiplied by 1/Delta?
    last_every_second = every_second

    output, recurrent_state, aux_input = (I‚Çñ,), (last_every_second,), (t,)
    return output, recurrent_state, aux_input

class InputDriver_static:
    # produces each timestep
    # the idea was it actually provided the INPUT signal! (I‚Çñ)
    # input also drives the program flow !
    def simulate_input_and_drive_next_step(simargs1):

        last_every_second = None
        for k in range(simargs1.K):
            t = k * simargs1.Delta
            (I‚Çñ,), (last_every_second,), (t,) = input_I‚Çñ((last_every_second,), (t, simargs1.Delta,))
            yield k, t, [I‚Çñ,]


# comprised of multiple neurons
class FullModel:

    def __init__(self) -> None:
        """
        Generates an instance
        """

        BETA_RANGE = [0.9, 1.1]
        NEURONS = 20

        na = []
        for i in range(NEURONS):
            n = Neuron_static.n0_prot().copy()
            d = BETA_RANGE[1] - BETA_RANGE[0]
            n['beta'] = 1.1  # (np.random.rand() * d) + BETA_RANGE[0]
            na.append(n)

        self.na = na


# local loop-updating variables
# last_x‚Çñ = 0.0
# Nc = 0


# `deltaT` was:
#     1 * MSEC * 0.2 (when duration is specified)
#     1 * MSEC (when K is specified)
simargs1 = SimulatorArgs1.simargs_factory(duration=3.0, deltaT=1 * MSEC * 0.2)

simargs1.invar()

full_model = FullModel()

# Slow, hence, cache!
# neuron instance
class Neuron:
    def init_slow_cache(self, n_obj):
        _tau = Neuron_static.get_neuron_tau(n_obj, DELTA0)
        _rho_corrected = Neuron_static.get_neuron_rho(_tau, simargs1.Delta)
        _sigma_eps_corrected = full_model.na[0]['sigma_eps'] * \
            math.sqrt(simargs1.Delta/DELTA0)
        print("_rho_corrected = ", _rho_corrected, "rho=", full_model.na[0]['rho'])
        print("_sigma_eps_corrected = ", _sigma_eps_corrected,
                "sigma_eps=", full_model.na[0]['sigma_eps'])

        self._sigma_eps_corrected = _sigma_eps_corrected
        self._rho_corrected = _rho_corrected

def new_list_1d(size1):
    return [None] * size1

def new_list_2d(size1, size2):
    # rows x columns
    out = []
    for i1 in range(size1):
        row = []
        for i2 in range(size2):
            row.append(None)
        out.append(row)
        assert len(row) == size2
    assert len(out) == size1
    return out

# [neuron_id]
NEURONS_NUM = 1
TRIALS_NUM = 10
if True:
    tŒû = np.full((simargs1.K,), np.nan)

    x_ŒûŒû = np.full((NEURONS_NUM, simargs1.K,), np.nan)
    xlogpr_ŒûŒû = np.full((NEURONS_NUM, simargs1.K,), np.nan)
    N·∂ú_ŒûŒû = np.full((NEURONS_NUM, simargs1.K,), 99999, dtype=int)
    fire_probabilityŒûŒû = np.full((NEURONS_NUM, simargs1.K,), np.nan)
    Œª_ŒûŒû = np.full((NEURONS_NUM, simargs1.K,), np.nan, dtype=float)
    I‚Çñ_ŒûŒû = np.full((NEURONS_NUM, simargs1.K,), np.nan)

    # output.
    # Non-square. Hence, list of nparrays
    # œü_timesœü_Œû = new_list_1d(NEURONS_NUM)
    œü_timesœü_ë¥†Œû = new_list_2d(NEURONS_NUM, TRIALS_NUM)

    # Œõ_at_spikes_Œû = new_list_1d(NEURONS_NUM)
    Œõ_at_spikes_ë¥†Œû = new_list_2d(NEURONS_NUM, TRIALS_NUM)

    # local loop-updating variable(s) / the recurrent state
    N·∂ú_Œæ = np.zeros((NEURONS_NUM,))
    last_x‚Çñ_Œæ = np.zeros((NEURONS_NUM,))
    # last_x‚Çñ = 0.0
    # Nc = 0

    # for neuron_id in range(1):
    neuron_id = 0
    neur_instance = [Neuron()]
    neur_instance[neuron_id].init_slow_cache(full_model.na[neuron_id])

for k, t, I‚Çñ_Œû in InputDriver_static.simulate_input_and_drive_next_step(simargs1):

    # the recurrent state: (last_x‚Çñ_Œæ, N·∂ú_Œæ,)
    # aka. the local loop-updating variables


    # print( t, k, I‚Çñ_Œû )

    neuron_id = 0
    n = full_model.na[neuron_id]

    is_first_step = (k == 0)

    # def STEP(..., is_first_step)


    # *************************
    # *  Neuron model
    # *************************
    if False:
        # x‚Çñ is the State
        eps‚Çñ = n['sigma_eps'] * np.random.randn()

        # dirac_factor = 7.0  # terrible. Why no refactory period?
        dirac_factor = 1.0

        #dirac_factor = 1.0 / simargs1.Delta
        # print( "dirac_factor,",dirac_factor )
        x‚Çñ = n['rho'] * last_x‚Çñ_Œæ[neuron_id] + n['alpha'] * \
            I‚Çñ_Œû[inp_id] * dirac_factor + eps‚Çñ  # Eq.1

    inp_id = 0
    if True:
        dirac_factor = 1.0
        eps‚Çñ = neur_instance[neuron_id]._sigma_eps_corrected * np.random.randn()
        x‚Çñ = neur_instance[neuron_id]._rho_corrected * last_x‚Çñ_Œæ[neuron_id] + \
            n['alpha'] * I‚Çñ_Œû[inp_id] * dirac_factor + eps‚Çñ  # Eq.1

    # last_x‚Çñ_Œæ should be outside a loop function
    # last_x‚Çñ = x‚Çñ
    last_x‚Çñ_Œæ[neuron_id] = x‚Çñ

    # xlp is x at ?
    xlp = n['mu'] + n['beta'] * x‚Çñ
    Œª‚Çñ = math.exp(xlp)  # Eq.2
    # What will guarantee that xlp < 0 ? i.e. probability < 1
    # Where is x reset?

    # *****************************
    # * Point process simulation
    # *****************************

    fire_probability = Œª‚Çñ * simargs1.Delta  # * 100
    fire = fire_probability > np.random.rand()

    #output = (x‚Çñ * simargs1.Delta) > np.random.rand()
    #Nc += output

    # total count
    N·∂ú_Œæ[neuron_id] += fire

    # set all (x‚Çñ, Nc, xlp),
    # not: ?( t, I‚Çñ)
    # t_arr:
    tŒû[k] = t

    x_ŒûŒû[neuron_id][k] = x‚Çñ
    N·∂ú_ŒûŒû[neuron_id][k] = N·∂ú_Œæ[neuron_id]
    I‚Çñ_ŒûŒû[inp_id][k] = I‚Çñ_Œû[inp_id]
    del x‚Çñ, I‚Çñ_Œû

    xlogpr_ŒûŒû[neuron_id][k] = xlp
    del xlp

    fire_probabilityŒûŒû[neuron_id][k] = fire_probability
    Œª_ŒûŒû[neuron_id][k] = Œª‚Çñ

    if is_first_step:
        Neuron_static.report_neuron(n, simargs1.Delta)

    # del x_arr,     xlogpr_arr,    Nc_arr,    fire_probability_arr,    Œª_arr,    I_arr,
    del Œª‚Çñ, fire_probability, t, fire

    # keep the recurrent state: (aka. the local loop-updating variables)
    # del last_x‚Çñ_Œæ, N·∂ú_Œæ

print("Simulation time = T =", simargs1.T, ". Mean rate = ",
      N·∂ú_ŒûŒû[:][-1].astype(float)/simargs1.T, "(spikes/sec)")

for neuron_id in range(1):
    print("Integral of Œª = ", np.sum(Œª_ŒûŒû[neuron_id]) * simargs1.Delta)
    print("Mean Œª = ", np.sum(Œª_ŒûŒû[neuron_id]) * simargs1.Delta / simargs1.T)


def cumsum0(x, startv=0.0,  cutlast=True):
    """
        Numerical integration.
        generates a cumsum that starts with 0.0,
        of the same size as x
        To maintain the size, it removes the last element,
        and returns it separately.
        """
    c = np.cumsum(x)
    maxval = c[-1]
    c = np.concatenate((np.array([startv]), c))
    if cutlast:
        c = c[:-1]

    return c, maxval


def generate_Œõ_samples_unit_exp1(total_rate):
    """
    Generates samples from
    Exponential distribution
    Œª = 1.0

    PDF(x) = Œª exp(-Œªx)

    Name evolution: Old names:
        generate_unit_isi
        generate_isi_samples_unit_exp1
        generate_Œõ_samples_unit_exp1

    where x = ISI in temrs of "virtual-time" (Œõ)
    (Not really ISI: but ISI-Œõ )

    Enough number of samples to fit the whole `total_rate` (Œõ)

    `total_rate` units are in "virtual-time" (rate, Œª, intensity-integrated, capital Lambda: Œõ )
    ISIŒõ: Inter-spike inter-val -> inter-Œõ-val
    interval implies physical "time". But this is virtual-time (Œõ)

    True-time (internal, canonical, natural!)

    ISI = TR(R)

    time_quantiles01 -> Œõ_quantiles
    By q01 I meant the ISI in terms of this.
    Max of sum(q01) is Sum(Œª) = (disrete)Sum(Œõ) = Œõ
    Evolution:
        Sum(Œª) = Œõ
        (disrete)Sum(Œª) = Œõ
        Sum(Œª) = (disrete)Sum(Œõ) = Œõ
        ‚à´Œª = (disrete)Sum(ŒîŒõ) = Œõ
        ‚à´Œª = Œ£(ŒîŒõ) = Œõ
        ‚à´Œª dt = Œ£(ŒîŒõ) = Œõ

    But here:
        Œõ = Œ£ (isi?)
        Œõ = Œ£(ŒîŒõ)

        isi? = ŒîŒõ
    On the other hands, the units of Œª are 1/time?

    No, in fact, the time is added to it by the integration:
        ‚à´Œª dt  is unit-less
        Hence, Œª is 1/t or better to say: 1/Œît?
    I am adding back:
        1. physical Dim
        2. Œî or whole (Œ£). Œ£Œî = whole. Œ£Œî = id
            Œî = Œ£^{-1} * id ?!
            Œî = 1/Œ£ * id
            Œî = id/Œ£ (but in opposite order)

        3. Adding back the natures other than time. e.g. Œõ-ness.

        4. Maybe: DIM of `rand()`. (?)

        5. log-ness as DIM. log-ness is important.
            log-transforms changre DIM. (BEcause they are in sa differnt space!)

            Œ£,Œî are kind of transformations:    the results are relative/absolute (to reference points).
            jens (material) is different.
            This is a very fundamental quality.

            Œ£ ( seq1 ) = cusum
            Œî (seq2) = diff
            Œ£(Œî) is obviously are transfomrations.

        6. Nature of space (of transformation)
            eg rand is in its own "space".


    The `rand()` itself has some DIM (?)

    The idea is to keep track of the dimentionality, so that you dont need to remember what transformations are done inside.
    So the output "type" is about interface: to avoid the inside.
    But to give sufficient info about inside.

    The language of input/output needs "type".

    So what is the type of output here?

    Œ£
    (
        desribe type in terms of the transfomrations. a "composition" of them.
        A composition is an architecture. I the shape of the circuit.
        DIMension is about the general shape of ciruit - in that sense.
        A homeomorphism of the circuit itself?
        The black box has in its outputs a homemorphism of inside?
        (But hiding certain aspects of inside. Not all)
    )
    Œ£(x)
    Œ£(Œî)
    Œ£(x = Œî(.)) =  // not to be confuse with a function x=>
    Œ£(x = (Œî=(‚à´Œª dt))) =
    Œ£(Œî=(‚à´Œª dt))
    // ^ This also confirms that what is inside the Œ£ is a Œî. Hence, we end of having a Œ£(Œî) or Œ£Œî.
    Œ£(Œî: ‚à´Œª dt)
    Œ£(Œî: ‚à´Œª dt st. input = Œî)
    By input I mean rand !
    But "Œ£(Œî: ‚à´Œª dt)" is the charactger ofd the type.
        It is a whole picture. And you mark the inpt and output there.
    "output" Œ£("input" Œî: ‚à´Œª dt)
    Then attach: (Occam: IO language: a? a!)
    "input"! = `rand()`   # attach
    Occam language:
    "output!" Œ£("input?" Œî: ‚à´Œª dt)
    In fact !? already sa it is output or input. ut we still use "" (verbal) labels.
    "output!" Œ£("input?" Œî: ‚à´Œª dt)

    "output"! Œ£("input"? Œî: ‚à´Œª dt)  "input"! `rand`
    "output"! Œ£("input"? Œî: ‚à´Œª dt)  "input"!: `rand`
    *  "input"!: `rand` <---- simlar pattern to: (Œî: ...)
    *  `rand` "input"!  <--- input outputs the previous expression
    label, after or before?
    "After-" notation:
        Œ£("input"? Œî: ‚à´Œª dt) "output"! ;  `rand` "input"!
        Œ£("input"? Œî: ‚à´Œª dt) -> "output"! ;  `rand` -> "input"!

    "After-" notation: for bother direcitons of input/output?
        Œ£("input"? <- Œî: ‚à´Œª dt) -> "output"! ;  `rand` -> "input"!
        Œ£("input"? -> Œî: ‚à´Œª dt) -> "output"! ;  `rand` -> "input"!

        (Œ£( Œî: ‚à´Œª dt) -> "output"! , "input"? ->Œî) ;  `rand` -> "input"!

    Full picture:
        Œ£( Œî: ‚à´Œª dt)
        Œ£( Œî: (‚à´Œª dt) = "input": `rand`)
        Œ£( Œî? (‚à´Œª dt) = input? `rand`)
        Œ£( Œî?! (‚à´Œª dt) = input?! = `rand`)
        Œ£( (‚à´Œª dt)  =  Œî?! = input?! = `rand`)
        Œ£( Œî?!  = (‚à´Œª dt) = input?! = `rand`)

    How time is eliminated?
            Œ£( Œî?!  = (‚à´ (Œª?) dt) = input?! = `rand`)
    Œª? ---> Œª is not geivern here
    :  -----> as
    =  -----> as
    ?!  -----> as, attached immediately.

    Œ£( Œî  = (‚à´ (Œª= 1/ISI) dt) = input = `rand`)
    not sure about ISI here
    Where is ISI? In fact:
    ISI is closely related to Œî.
    ISI = time-quantile (Œî)?
    NO, we need the map of almbda.
    Yes we do have it.

    time = inv(Œõ(¬∑))

    I didn't utilise ‚àò !

    Œ£ ‚àò Œî = (‚à´dt) ‚àò (Œª(¬∑)?),  ... = `rand` := input
    aha:
    Œ£ ‚àò (Œî = (‚à´dt) ‚àò (Œª(¬∑)?)),  ... = `rand` = input
    ‚àò means apply.
    Œ£ ‚àò (Œî = (‚à´dt) ‚àò (Œª?‚àò(¬∑)))

    Œ£ ‚àò (Œî := (‚à´dt) ‚àò (Œª?‚àò(¬∑)))

    Now the meaning of / correct name of this function is clear
    instead of ISI, we should say, Œõ
    Œõ_quantiles = generate_Œõ_samples_unit_exp1(maxŒõ)
    was: Œõ_quantiles = generate_isi_samples_unit_exp1(maxŒõ)
    was: time_quantiles = generate_isi_samples_unit_exp1(maxŒõ)
    was: quantiles01 = generate_unit_isi(maxv)

    simply from a mythtake:! st += isi
    Important:
        isi <-> ŒîŒõ

    """
    """
    st -> sŒõ
    sŒõ = sumŒõ = Œõ

    quantiles01 <-> Œõ
    aks: time_quantiles01 (which is totally wrong!)

    Correspondance: Œõquantiles <-> spike_timesœü

    spike_times_Al -> œü_times_Œæ -> œü_times_Œû -> œü_timesŒæ_Œû? -> œü_timesœü_Œû (good: œü indicates some kind of array suffix (redunndancy in the name that specifies the type)). Also it is not soft (list)

    Œõ_at_spikes_Al -> Œõœü_Œæ -> Œõ_atœü_Œæ  -> Œõ_at_spikes_Œæ -> Œõ_at_spikes_Œû

    Œõ_quantiles -> Œõ_atœü ->? Œõ_at_spikes

    t_arr -> tŒû  (should I use t_Œû instead?)
    """
    # print( "ISI(%g):"%(total_rate), end='')
    Œõ = 0.0
    Œõa = []
    while Œõ < total_rate:
        if Œõ > 0.0:
            Œõa.append(Œõ)
        ŒîŒõ = -math.log(np.random.rand())
        # print( ŒîŒõ, Œõa, end='')
        # Œõa.append(ŒîŒõ)
        Œõ += ŒîŒõ
        # if Œõ > total_rate:
        #    break
    # print()
    return np.array(Œõa)

# Œª_arr is already small. -> Œª_Œû

# Generates a Point Process

def generates_time_points(Œª_Œû, ŒîT, tŒû):
    # tŒû
    #ŒõcumintegrŒª_Œû, maxŒõ = cumsum0(Œª_ŒûŒû[neuron_id], cutlast=False)*simargs1.Delta
    #t_arr_aug = np.concatenate(tŒû, np.array([tŒû[-1]+simargs1.Delta]))
    #ŒõcumintegrŒª_Œû, _ = cumsum0(Œª_ŒûŒû[neuron_id], cutlast=True)*simargs1.Delta
    cumintegrŒª_ŒûŒæ2, _ignore_max = cumsum0(Œª_Œû, startv=0.0, cutlast=True)
    ŒõcumintegrŒª_Œû = cumintegrŒª_ŒûŒæ2 * ŒîT
    # ŒõcumintegrŒª_Œû = Œõ(t) = Œõt   Œõt_arr
    # todo: find a unicode substitute for `_arr` suffix.

    maxŒõ = ŒõcumintegrŒª_Œû[-1]
    assert ŒõcumintegrŒª_Œû[-1] == np.max(ŒõcumintegrŒª_Œû), "monotonically increaseing"

    # time_reversal_transform

    # Time-Rescaling: Quantile ~ (physical) time (of spikes)
    # todo: rename Œõ_quantiles
    # Œõ_quantiles is ...
    # Converts Œõ -> time. time(Œõ)
    time_rescaling_interp_func = interp1d(ŒõcumintegrŒª_Œû, tŒû, kind='linear')
    Œõ‚Å±‚Åø·µõ = time_rescaling_interp_func
    # (x,y, ...)  y = F(x).  tŒû = F(ŒõcumintegrŒª_Œû)
    # time_rescaling_interp_func: Œõ -> t
    # Hence, the opposiute of Œõ(t)
    # t(Œõ)  tŒõ
    # => `time_rescaling_interp_func` IS the Time-Rescaling transformation function
    # It is a continuous function
    # It is in fact better called Œõ_rescaling
    #     But its standard Mathematical name is Œõ_rescaling
    #     It is about Time-rescaling "Theorem"
    # Converts Œõ -> time. time(Œõ)
    # In a sense, rescaling means calculating "quantile"s. Hence the name: Œõquantiles (spike_timesœü), spike_Œõs
    #    Œõquantiles, spike_Œõs, Œõ_at_spikes, Œõ_at_points, Œõ_points (time_points)
    #   time_of_spikes, Œõ_of_spikes
    # Œõquantiles -> Œõ_at_spikes
    # However, note that it is about uotput spikes

    # Time-rescaled quantiles:
    #Œõ_quantiles = np.arange(0,maxŒõ,maxŒõ/10.0 * 10000)
    Œõ_atœü = generate_Œõ_samples_unit_exp1(maxŒõ)
    # print( Œõ_atœü )

    # empty_spikes, empty_spiketrain, no_spikes
    no_spikes = Œõ_atœü.shape[0] == 0
    assert no_spikes or \
        np.max(ŒõcumintegrŒª_Œû) >= np.max(Œõ_atœü)
    assert no_spikes or \
        np.min(ŒõcumintegrŒª_Œû) <= np.min(Œõ_atœü)
    if no_spikes:
        print("Warning: empty spike train. *****")

    spike_timesœü = time_rescaling_interp_func(Œõ_atœü)
    spike_timesœü = Œõ‚Å±‚Åø·µõ(Œõ_atœü)
    # why changed to this?
    #spike_timesœü = time_rescaling_interp_func(ŒõcumintegrŒª_Œû)

    del maxŒõ, ŒõcumintegrŒª_Œû
    # del spike_timesœü, Œõ_atœü
    assert spike_timesœü.shape == Œõ_atœü.shape
    return Œõ_atœü, spike_timesœü


for trial in range(TRIALS_NUM):
    # Œõ_quantiles
    Œõ_atœü, spike_timesœü = \
        generates_time_points(Œª_ŒûŒû[neuron_id], simargs1.Delta, tŒû)

    # based on stackoverflow.com/questions/19956388/scipy-interp1d-and-matlab-interp1
    # spikes = (spike_timesœü, Œõ_atœü)  # spikes and their accumulated Œõ

    # œü_times_Œû <- œü_times_Œæ = spike_times_Al
    œü_timesœü_ë¥†Œû[neuron_id][trial] = spike_timesœü
    # Œõ_at_spikes_Œæ = Œõ_atœü_Œæ = Œõœü_Œæ = Œõ_at_spikes_Al
    Œõ_at_spikes_ë¥†Œû[neuron_id][trial] = Œõ_atœü
    del spike_timesœü, Œõ_atœü

    # todo: (Œõ_at_spikes_Œæ) Œõ_at_spikes_Œû -> Œõ_atœüŒæ ? or Œõ_atœü_Œæ ?  or Œõœü_Œæ ?
    assert len(œü_timesœü_ë¥†Œû) == len(Œõ_at_spikes_ë¥†Œû), "number of neurons (PP channels) should match"
    assert len(œü_timesœü_ë¥†Œû[neuron_id]) == len(Œõ_at_spikes_ë¥†Œû[neuron_id]), "number of trials should match"
    # remove this line later:
    #print( œü_timesœü_ë¥†Œû[neuron_id][trial].shape , Œõ_at_spikes_ë¥†Œû[neuron_id][trial].shape )
    assert œü_timesœü_ë¥†Œû[neuron_id][trial].shape == Œõ_at_spikes_ë¥†Œû[neuron_id][trial].shape

simulation_result = \
    (tŒû, x_ŒûŒû, xlogpr_ŒûŒû, Œª_ŒûŒû, œü_timesœü_ë¥†Œû, Œõ_at_spikes_ë¥†Œû, fire_probabilityŒûŒû, N·∂ú_ŒûŒû, I‚Çñ_ŒûŒû)

# import sys
# sys.path.append('/ufs/guido/lib/python')
from sim2_plot import *

plot_all(simargs1, full_model.na, Neuron_static.get_neuron_tau, simulation_result, DELTA0, simargs1.Delta)
